{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying and Natural Language Processing with Yelp Reviews Data\n",
    "#### W207 Section 3, Group - <span style=\"color:orange\"><strong>C</strong></span>olors\n",
    "#### Summer, 2018\n",
    "#### Team members:\n",
    "- Chandra Sekar, chandra-sekar@ischool.berkeley.edu\n",
    "- Guangyu (Gary) Pei, guangyu.pei@ischool.berkeley.edu\n",
    "- Jooyeon (Irene) Seo, jooyeon@ischool.berkeley.edu\n",
    "- Sijie (Anne) Yu, syu.anne@berkeley.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Config Jupyter session\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Global configurations\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# Config system logs\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals\n",
    "Our projectâ€™s primary concept is to utilize Yelp data (from kaggle) to rate new business. That is, we are going to get Yelp user review data, use review texts to predict review is **positive** or **negative**. When people talk about a new business, we can capture their words, fit into the model, then predict its rating, sort of understand its quality and potential.\n",
    "\n",
    "#### The Yelp Review Dataset\n",
    "We write a shell [script](https://github.com/annesjyu/m207_summer_2018) to select $10,000$ reviews for training, testing and dev respectively, each set consisting in 50% negative and 50% positive reviews. We keep only review text and stars columns, then binarize stars into target label: \n",
    "- if starts >= $3.0$, review is *positive*\n",
    "- otherwise, it's *negative*.\n",
    "\n",
    "The following code will load the dataset and split it into $3$ sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data dim:  (29701, 2)\n",
      "train data dim:  12000\n",
      "test data size:  10000\n",
      "dev data size:  7700\n",
      "________________________________________________________________________________\n",
      "Training examples:\n",
      " [\"I was visiting Phoenix and Yelped a few places to try. My friend and I decided to go to Cornish Pasty in Tempe. It was dark and open seating  definitely had a pub vibe inside. We decided to sit outside at the patio to enjoy the Fall weather.   I got the Oggie and my friend got Shepard's pie with a side Oven Chips - added the garlic and jalepenos. I love anything pie crust and these pasties definitely hit the spot.  The minced lamb from the Shepard's pie was  cooked perfectly and not too gamey. The Oggie was a classic and a comfort food. Oven chips were like french fries cooked with minced garlic and jalepenos that weren't spicy at all. I found out there's a location in Vegas and will definitely have to visit there to try the other pasties. \"\n",
      " \"Went inside and ordered a DOUBLE JACK COMBO and several other items to go.  Got home and found there was only one burger patty in the DOUBLE jack.  Since we live about five miles from the restaurant  it wasn't worth the drive back.  It was probably an accident but if you wanted to rip-off a customer  this would be an easy way to do it.  The're are plenty of other burger joints in Surprise so I don't need Jack In The Box anymore. \"]\n",
      "[1 0]\n",
      "________________________________________________________________________________\n",
      "Testing examples:\n",
      " [\"Yuck. Total waste of money. Based on reviews bought two chocolate chip cookies at $5 a piece. Got back to the hotel all excited for a late night treat and was disappointed. Cookies were hard!!  Overlooked!!  Crunchy!!!  Could not even eat them. You are better off going to McDonald's and buying those at 3 for a $1.00  truth! We did the next night (tonight) and they baked them on the spot. We got soft and warm cookies cheap and a million times better than this places boo boo.  No complaints about service. Nice peeps  but that doesn't make up for crappy  rock hard cookies. \"\n",
      " 'Valley Metro:  When you go on strike  do you know who you\\'re fucking over?  Poor people who have no other way to get to work.  What are you striving for each time you strike?  Your drivers are consistently rude and nasty and your buses are always late.  Whenever a bus arrives on time  it\\'s probably actually the bus that should have been arriving 20 minutes ago.  Since the last time  nothing in the way of service had improved.  Rather  it had gotten worse.  Do you know how many times an Orbit in Tempe has almost run me over when I\\'m riding my bike?  When my girlfriend called to complain about it  the operator said something like  \"\"Oh  him.  We know all about HIM!\"\" Yet  he was allowed to work there and continue endangering the lives of how many people?  Ridiculous.  Until you can get the buses to run on time and not treat customers like shit (seriously  the way you guys treat riders sometimes is ridiculous; my ass would get fired for that in a damn minute)  please don\\'t abandon the people who NEED and REQUIRE your service.  I understand working conditions suck  you have to drive crackheads and everything  but fuck.  You guys are the worst.  You\\'re mean and you\\'ve left so many people high and dry again and again and again. ']\n",
      "[0 0]\n",
      "________________________________________________________________________________\n",
      "Dev examples:\n",
      " ['Rat hole! This place used to be great 10-15 years ago. My first non-smoking room was filled with cigar smoke  the second - bong water. The jets from McCarran sound like they are landing on the pool. They beautiful lawns are being replaced by astro-turf  and the remaining grass is mostly dead. The room was a mix of atrocious crappy furniture (black laminate pressed stuff  rotting from water damage. Windows did not work at all. Very depressing indeed. '\n",
      " 'I stayed here one time with few friends.. DID NOT ENJOY IT  the location is off the strip. a little bit far from everything. the area looks a little bit ghetto  the room was old/smells too.  Casino of course is old/smells too. I pretty much did not stay unless i really wanted to sleep. ']\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "with np.warnings.catch_warnings():\n",
    "    # There are some bad data, we just dont want to see a lot of warning messages\n",
    "    np.warnings.filterwarnings('ignore', r'Some errors were detected')\n",
    "    data = np.genfromtxt('data.csv',dtype='str', delimiter='|', skip_header=1, \n",
    "                         usecols = (0,1), invalid_raise=False, loose=True)\n",
    "\n",
    "    print (\"Full data dim: \", data.shape)\n",
    "    \n",
    "    # Shuffle the data, each dataset will have roughly the same number of examples for each label.\n",
    "    shuffle = np.random.permutation(np.arange(data.shape[0]))\n",
    "    X, Y = data[shuffle, 0], data[shuffle, 1]    \n",
    "    \n",
    "    train_data, train_labels = X[0:12000], Y[:12000].astype(np.int)\n",
    "    test_data, test_labels = X[12000:22000], Y[12000:22000].astype(np.int)\n",
    "    dev_data, dev_labels = X[22000:-1], Y[22000:-1].astype(np.int)\n",
    "\n",
    "    NUM_OF_TRAINING_DATA = len(train_data)\n",
    "    NUM_OF_TESTING_DATA = len(test_data)\n",
    "    NUM_OF_DEV_DATA = len(dev_data)\n",
    "\n",
    "    print ('train data dim: ', NUM_OF_TRAINING_DATA)\n",
    "    print ('test data size: ', NUM_OF_TESTING_DATA)\n",
    "    print ('dev data size: ', NUM_OF_DEV_DATA)\n",
    "    print ('_'*80)\n",
    "    print ('Training examples:\\n', train_data[0:2])\n",
    "    print (train_labels[0:2])\n",
    "    print ('_'*80)\n",
    "    print ('Testing examples:\\n', test_data[0:2])\n",
    "    print (test_labels[0:2])\n",
    "    print ('_'*80)\n",
    "    print ('Dev examples:\\n', dev_data[0:2])\n",
    "    print (dev_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze train, dev and test datasets to find out data distributions. Ideally we want to have 50% examples for either label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive train data:  6011 , negative train data:  5989\n",
      "positive test data:  4990 , negative test data:  5010\n",
      "positive dev data:  3862 , negative dev data:  3838\n"
     ]
    }
   ],
   "source": [
    "print ('positive train data: ', len(np.where(train_labels==1)[0]), \n",
    "       ', negative train data: ', len(np.where(train_labels==0)[0]))\n",
    "print ('positive test data: ', len(np.where(test_labels==1)[0]), \n",
    "       ', negative test data: ', len(np.where(test_labels==0)[0]))\n",
    "print ('positive dev data: ', len(np.where(dev_labels==1)[0]), \n",
    "       ', negative dev data: ', len(np.where(dev_labels==0)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing\n",
    "\n",
    "We will create doc-term matrix from data, so can fit all classifiers. There are a couple of steps of doing it.\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a baseline using default CountVectorizer and NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 5303)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.87      0.81      3372\n",
      "          1       0.89      0.79      0.84      4328\n",
      "\n",
      "avg / total       0.83      0.83      0.83      7700\n",
      "\n",
      "top 100 terms:\n",
      " ['kind', 'stars', 'different', 'favorite', 'probably', 'took', 'perfect', 'dinner', 'table', 'worth', 'hot', 'sweet', 'vegas', 'inside', 'home', 'bad', 'need', 'meal', 'drinks', 'salad', 'excellent', 'work', 'feel', 'quite', 'awesome', 'tried', 'happy', 'overall', 'cheese', 'long', 'location', 'thing', 'clean', 'looking', 'price', 'prices', 'wasn', 'big', 'lunch', 'sauce', 'night', 'super', 'eat', 'times', 'new', 'bar', 'lot', 'say', 'want', 'bit', 'fresh', 'day', 'll', 'wait', 'going', 'sure', 'small', 'experience', 'know', 'area', 'way', 'think', 'right', 'better', 'chicken', 'went', 'recommend', 'amazing', 'order', 'restaurant', 'didn', 'came', 'people', 'did', 'menu', 'make', 'ordered', 'pretty', 'delicious', 'come', 'got', 'staff', 'try', 'definitely', 'little', 'love', 'friendly', 'best', 'don', 've', 'nice', 'really', 'time', 'just', 'service', 'like', 'food', 'great', 'place', 'good']\n",
      "________________________________________________________________________________\n",
      "lest important 100 terms:\n",
      " ['owed', 'billing', 'expired', 'threatened', 'ripoff', 'incompetent', 'dissatisfied', 'flagged', 'disrespected', 'dispute', 'compensation', 'rudest', '800', 'operate', 'dishonest', 'disgusted', 'nur', 'cox', 'horrendous', 'refusing', 'canceling', 'liars', 'subpar', 'racist', 'blatantly', 'blamed', 'ignorant', 'denied', 'wouldnt', 'actions', 'arrogant', 'reads', 'insult', 'shaking', 'demanded', 'appalled', 'crawling', 'mush', 'supervisor', 'scum', 'placing', 'pathetic', 'proceed', 'refunded', 'drain', 'refunds', 'counted', 'uns', 'sarcastic', 'scam', 'determined', 'indifferent', 'abysmal', 'wilted', 'valid', 'unappetizing', 'slowest', 'insulted', 'tape', 'als', 'downhill', 'voicemail', '21st', 'approved', 'letters', 'reported', 'defensive', 'clarify', 'threatening', 'billed', 'nach', 'belongings', 'belong', 'canceled', 'pouring', 'tenants', 'grossed', 'argue', 'vomit', 'headache', 'quit', 'wasting', 'sheets', 'false', 'individuals', 'notified', 'periods', 'attempts', 'hr', 'joking', 'redeeming', 'statement', 'inattentive', 'meantime', 'application', 'patron', 'contacting', 'ur', 'costing', 'confirmed']\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer(strip_accents='ascii', stop_words='english', min_df=0.001)\n",
    "train_dtm = v.fit_transform(train_data)\n",
    "print (train_dtm.shape)\n",
    "\n",
    "bnb = BernoulliNB(alpha=0.01)\n",
    "bnb.fit(train_dtm, train_labels)\n",
    "predicted = bnb.predict(v.transform(dev_data))\n",
    "\n",
    "print (classification_report(predicted, dev_labels))\n",
    "\n",
    "terms = v.get_feature_names()\n",
    "top100 = np.argsort(bnb.coef_[0])[-100:]\n",
    "print ('top 100 terms:\\n',[terms[int(w)] for w in top100])\n",
    "print ('_'*80)\n",
    "bottom100 = np.argsort(bnb.coef_[0])[:100]\n",
    "print ('lest important 100 terms:\\n',[terms[int(w)] for w in bottom100])\n",
    "print ('_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
